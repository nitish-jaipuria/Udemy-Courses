{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Build your own spam detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4600, 58)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv ( r'F:/DATA SCIENCE/Udemy/Lazy Programmer Codes/nlp_class/spambase.data' ).as_matrix()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[: , :48]\n",
    "Y = data[: , -1]\n",
    "\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Rate :  0.89\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print ( \"Classification Rate : \", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Rate :  0.94\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print ( \"Classification Rate : \", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Sentiment Analysis in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set( w.rstrip() for w in open( r'F:/DATA SCIENCE/Udemy/Lazy Programmer Codes/nlp_class/stopwords.txt' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitish\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Nitish\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "positive_reviews = BeautifulSoup( open( r'F:/DATA SCIENCE/Udemy/Lazy Programmer Codes/nlp_class/electronics/positive.review' ).read())\n",
    "positive_reviews = positive_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitish\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Nitish\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "negative_reviews = BeautifulSoup( open( r'F:/DATA SCIENCE/Udemy/Lazy Programmer Codes/nlp_class/electronics/negative.review' ).read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to balance classes remove excess positive reviews\n",
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index_map = {}\n",
    "current_index = 0\n",
    "\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [ t for t in tokens if len(t) > 2 ]\n",
    "    tokens = [ wordnet_lemmatizer.lemmatize(t) for t in tokens ]\n",
    "    tokens = [ t for t in tokens if t not in stopwords ]\n",
    "    return(tokens)\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1)\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum()\n",
    "    x[-1] = label\n",
    "    return(x)\n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "data = np.zeros( ( N, len(word_index_map) + 1 ) )\n",
    "\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens,1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "    \n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens,0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "    \n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Rate :  0.76\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "\n",
    "print(\"Classification Rate : \", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommend 0.707745558383\n",
      "memory 0.968073533779\n",
      "card -0.61323300238\n",
      "time -0.53426041347\n",
      "wa -1.59332433715\n",
      "perfect 1.00131542981\n",
      "picture 0.617865332471\n",
      "unit -0.63758900665\n",
      "expected 0.536954931484\n",
      "n't -2.10426609708\n",
      "you 1.09425593195\n",
      "buy -0.89979593573\n",
      "pretty 0.734159951149\n",
      "return -1.11864656115\n",
      "quality 1.3463306804\n",
      "bad -0.75021214785\n",
      "using 0.582630802939\n",
      "week -0.692866519356\n",
      "look 0.530768241494\n",
      "speaker 0.746991608472\n",
      "doe -1.25473538039\n",
      "price 2.73198951137\n",
      "'ve 0.711045968867\n",
      "highly 1.00928355649\n",
      "ha 0.654121256476\n",
      "excellent 1.1514568555\n",
      "tried -0.737049057418\n",
      "paper 0.605521420282\n",
      "fast 0.859333108815\n",
      "cable 0.65707258778\n",
      "lot 0.726713979772\n",
      "easy 1.77032371407\n",
      "support -0.875286662718\n",
      "space 0.576105687709\n",
      "money -1.08566673061\n",
      "then -1.014893976\n",
      "love 1.15798006431\n",
      "month -0.791614375306\n",
      "comfortable 0.672198154045\n",
      "bit 0.640178094671\n",
      "value 0.567970348459\n",
      "sound 1.06939770556\n",
      "home 0.53964547643\n",
      "little 0.79689666349\n",
      "video 0.603595745212\n",
      "fit 0.516472379457\n",
      "pro 0.517785202908\n",
      "try -0.698361844297\n",
      "happy 0.586659162184\n",
      "returned -0.820890200134\n",
      "refund -0.563514373051\n",
      "poor -0.754426203984\n",
      "warranty -0.569698547811\n",
      "hour -0.563256577286\n",
      "item -1.00218408274\n",
      "static -0.501969435579\n",
      "customer -0.674923384287\n",
      "company -0.511568767433\n",
      "returning -0.53762389624\n",
      "junk -0.550763892431\n",
      "waste -1.03050802776\n",
      "stopped -0.529104052879\n",
      "terrible -0.510008036956\n"
     ]
    }
   ],
   "source": [
    "# find which words are important\n",
    "\n",
    "threshold = 0.5\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < - threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) NLTK Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bob', 'NNP'), ('is', 'VBZ'), ('great', 'JJ')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag('Bob is great'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wolv\n",
      "wolf\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem('wolves'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('wolves'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recongnition\n",
    "s = \"Nitish Jaipuria was born on 17th september, 1988\"\n",
    "\n",
    "tags = nltk.pos_tag(s.split())\n",
    "nltk.ne_chunk(tags).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "titles = [ line.rstrip() for line in open(r'F:/DATA SCIENCE/Udemy/Lazy Programmer Codes/nlp_class/all_book_titles.txt')]\n",
    "stopwords = set ( w.rstrip() for w in open(r'F:/DATA SCIENCE/Udemy/Lazy Programmer Codes/nlp_class/stopwords.txt'))\n",
    "\n",
    "stopwords = stopwords.union({\n",
    "    'introduction', 'edition', 'series', 'application',\n",
    "    'approach', 'card', 'access', 'package', 'plus', 'etext',\n",
    "    'brief', 'vol', 'fundamental', 'guide', 'essential', 'printed',\n",
    "    'third', 'second', 'fourth', })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer_no(s):\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [ t for t in tokens if len(t) > 2 ]\n",
    "    tokens = [ wordnet_lemmatizer.lemmatize(t) for t in tokens ]\n",
    "    tokens = [ t for t in tokens if t not in stopwords ]\n",
    "    tokens = [ t for t in tokens if not any(c.isdigit()) for c in t ] # removing numbers as well\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index_map = {}\n",
    "current_index = 0\n",
    "all_tokens = []\n",
    "all_titles = []\n",
    "index_word_map = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for title in titles:\n",
    "    try:\n",
    "        title = title.encode('ascii', 'ignore')\n",
    "        all_titles.append(title)\n",
    "        tokens = my_tokenizer_no(title)\n",
    "        all_tokens.append(tokens)\n",
    "        for token in tokens:\n",
    "            if token not in word_index_map:\n",
    "                word_index_map[token] = current_index\n",
    "                current_index += 1\n",
    "                index_word_map.append(token)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_vector_nolabel(tokens):\n",
    "    x = np.zeros(len(word_index_map))\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] = 1\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(all_tokens)\n",
    "D = len(word_index_map)\n",
    "X = np.zeros((D,N))\n",
    "i = 0\n",
    "\n",
    "for token in all_tokens:\n",
    "    X[:,i] = tokens_to_vector_nolabel(token)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD()\n",
    "Z = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(Z[:,0], Z[:,1])\n",
    "for i in xrange(D):\n",
    "    plt.annotate(s=index_word_map[i], xy=(Z[i,0], Z[i,1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Spinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
